// path: nodes/storage.buckets.ts
/**
 * NODE: storage.buckets
 * Phase: execute
 *
 * Purpose:
 *  - Provision a private user uploads bucket and basic owner policies.
 *  - Provide server utilities for presigned upload/download URLs.
 *
 * Outputs:
 *  - artifacts/${orc}/repo/supabase/migrations/0002_storage.sql
 *  - artifacts/${orc}/repo/lib/storage.ts
 *
 * Notes:
 *  - Policies target 'storage.objects' with the 'owner' column (uuid).
 *  - Direct uploads use a presigned URL generated by a server-side key (service role).
 */

export type NodeId = string;
export type OrchestrationId = string;

export interface ExecutionContext {
  orchestrationId: OrchestrationId;
  correlationId: string;
  logger: { info(m: any): void; warn(m: any): void; error(m: any): void };
  storage: { saveArtifact: (path: string, content: string | Uint8Array) => Promise<void> };
}
export interface NodeSpec<I = unknown, O = unknown> {
  id: NodeId;
  phase:
    | "processResponses"
    | "analyze"
    | "validate"
    | "plan"
    | "execute"
    | "codeGeneration"
    | "integrate"
    | "finalize";
  estimate?: (input: I) => { tokens?: number; usd?: number };
  run: (input: I, ctx: ExecutionContext) => Promise<O>;
}

const lf = (s: string) => s.replace(/\r\n/g, "\n");

export const StorageBucketsNode: NodeSpec<unknown, { files: string[] }> = {
  id: "storage.buckets",
  phase: "execute",
  estimate: () => ({ tokens: 420, usd: 0.002 }),
  async run(_input, ctx) {
    const root = `artifacts/${ctx.orchestrationId}/repo`;

    const migration = lf(`-- Storage bucket + policies for user uploads (private by default)
-- Idempotent inserts to buckets table
insert into storage.buckets (id, name, public)
values ('user-uploads', 'user-uploads', false)
on conflict (id) do nothing;

-- Basic owner-based policies on storage.objects for this bucket
do $$
begin
  if not exists (
    select 1 from pg_policies
    where schemaname = 'storage' and tablename = 'objects' and policyname = 'Allow read own objects (user-uploads)'
  ) then
    create policy "Allow read own objects (user-uploads)"
      on storage.objects for select
      using (bucket_id = 'user-uploads' and (owner = auth.uid()));
  end if;

  if not exists (
    select 1 from pg_policies
    where schemaname = 'storage' and tablename = 'objects' and policyname = 'Allow insert own objects (user-uploads)'
  ) then
    create policy "Allow insert own objects (user-uploads)"
      on storage.objects for insert
      with check (bucket_id = 'user-uploads' and (owner = auth.uid()));
  end if;

  if not exists (
    select 1 from pg_policies
    where schemaname = 'storage' and tablename = 'objects' and policyname = 'Allow update own objects (user-uploads)'
  ) then
    create policy "Allow update own objects (user-uploads)"
      on storage.objects for update
      using (bucket_id = 'user-uploads' and (owner = auth.uid()));
  end if;

  if not exists (
    select 1 from pg_policies
    where schemaname = 'storage' and tablename = 'objects' and policyname = 'Allow delete own objects (user-uploads)'
  ) then
    create policy "Allow delete own objects (user-uploads)"
      on storage.objects for delete
      using (bucket_id = 'user-uploads' and (owner = auth.uid()));
  end if;
end
$$ language plpgsql;
`);

    const storageTs = lf(`// path: lib/storage.ts
// Server utilities for presigned upload/download URLs.
// Only import on the server.
import { createClient } from "@supabase/supabase-js";
import { env } from "../env.mjs";

const BUCKET = "user-uploads";

function serviceClient() {
  return createClient(env.SUPABASE_URL, env.SUPABASE_SERVICE_ROLE, { auth: { persistSession: false } });
}

/** Create a presigned upload URL for a user-owned path. */
export async function createPresignedUploadUrl(userId: string, filename: string) {
  const key = \`\${userId}/\${filename}\`;
  const supa = serviceClient();
  const { data, error } = await supa.storage.from(BUCKET).createSignedUploadUrl(key);
  if (error || !data) throw new Error(error?.message ?? "Failed to create signed upload URL");
  return { path: key, url: data.signedUrl };
}

/** Create a presigned download URL. */
export async function createPresignedDownloadUrl(path: string, expiresInSeconds = 60 * 10) {
  const supa = serviceClient();
  const { data, error } = await supa.storage.from(BUCKET).createSignedUrl(path, expiresInSeconds);
  if (error || !data) throw new Error(error?.message ?? "Failed to create signed download URL");
  return data.signedUrl;
}
`);

    const files = [
      { path: `${root}/supabase/migrations/0002_storage.sql`, content: migration },
      { path: `${root}/lib/storage.ts`, content: storageTs },
    ];

    for (const f of files) {
      await ctx.storage.saveArtifact(f.path, f.content);
    }

    ctx.logger.info({
      msg: "storage.buckets:written",
      files: files.map((f) => f.path),
      correlationId: ctx.correlationId,
    });

    return { files: files.map((f) => f.path) };
  },
};

export default StorageBucketsNode;
